# 资源   

## 数据集
1. [chinese_L-12_H-768_A-12.zip](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)
2. [albert_base_zh](https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz)   


## paper  
1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)  
2. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)     
3. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)   


## code   
1. bert: https://github.com/google-research/bert
2. distilbert: https://github.com/huggingface/transformers
2. albert: https://github.com/google-research/ALBERT

